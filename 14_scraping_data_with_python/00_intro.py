# we can go to any URL and append robots.txt to the URL and get all the
# allowed and not allowed scraping operations we can perform on that website
# for example, https://news.ycombinator.com/robots.txt

# to scrape a website
# 1. check if the websites provides an API for developers. For example: AirBnB API
# 2. use beautifulSoup library or scrapy framework (which has more features than beautifulSoup)


